{
    "docs": [
        {
            "location": "/",
            "text": "PyThaiNLP\n\n\nThai natural language processing in Python.\n\n\nPyThaiNLP is python module like nltk , but It's working with thai language.\n\n\nProject status\n\n\nDeveloping\n\n\nStable Version\n\n\n1.4\n\n\nDev  Version\n\n\n1.5\n\n\nLicense\n\n\nApache Software License 2.0",
            "title": "Home"
        },
        {
            "location": "/#pythainlp",
            "text": "Thai natural language processing in Python.  PyThaiNLP is python module like nltk , but It's working with thai language.",
            "title": "PyThaiNLP"
        },
        {
            "location": "/#project-status",
            "text": "Developing",
            "title": "Project status"
        },
        {
            "location": "/#stable-version",
            "text": "1.4",
            "title": "Stable Version"
        },
        {
            "location": "/#dev-version",
            "text": "1.5",
            "title": "Dev  Version"
        },
        {
            "location": "/#license",
            "text": "Apache Software License 2.0",
            "title": "License"
        },
        {
            "location": "/pythainlp-1-4-eng/",
            "text": "User manual PyThaiNLP 1.4\n\n\n\n\n\n\nUser manual PyThaiNLP 1.4\n\n\nAPI\n\n\nThai segment\n\n\nThai postaggers\n\n\nThai romanization\n\n\nSpell Check\n\n\npythainlp.number\n\n\nSort Thai text into List\n\n\nGet current time in Thai\n\n\nThai WordNet\n\n\nFind the most frequent words.\n\n\nIncorrect input language correction\n\n\nThai Character Clusters (TCC)\n\n\nEnhanced Thai Character Cluster (ETCC)\n\n\nThai Soundex\n\n\nThai meta sound\n\n\nThai sentiment analysis\n\n\nUtil\n\n\nngrams\n\n\n\n\n\n\nCorpus\n\n\nThai stopword\n\n\nThai country name\n\n\nTone in Thai\n\n\nConsonant in thai\n\n\nWord list in thai\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPI\n\n\nThai segment\n\n\nfrom pythainlp.tokenize import word_tokenize\nword_tokenize(text,engine)\n\n\n\n\ntext\n refers to an input text string in Thai.\n\n\nengine\n refers to a thai word segmentation system; There are 6 systems to choose from.\n\n\n\n\nicu (default) - pyicu has a very poor performance. \n\n\ndict - dictionary-based tokenizer. It returns False if the message can not be wrapped.\n\n\nmm - Maximum Matching algorithm for Thai word segmentation.\n\n\nnewmm - Maximum Matching algorithm for Thai word segmatation. Developed by Korakot Chaovavanich (https://www.facebook.com/groups/408004796247683/permalink/431283740586455/)\n\n\npylexto - LexTo.\n\n\ndeepcut - Deep Learning based Thai word segmentation (https://github.com/rkcosmos/deepcut)\n\n\n\n\nOutput: ''list'' ex. ['\u0e41\u0e21\u0e27','\u0e01\u0e34\u0e19']\n\n\nExample\n\n\nfrom pythainlp.tokenize import word_tokenize\ntext='\u0e1c\u0e21\u0e23\u0e31\u0e01\u0e04\u0e38\u0e13\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a\u0e42\u0e2d\u0e40\u0e04\u0e1a\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19\u0e44\u0e17\u0e22\u0e23\u0e31\u0e01\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e20\u0e32\u0e29\u0e32\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14'\na=word_tokenize(text,engine='icu') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d', '\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01', '\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19', '\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32', '\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19', '\u0e40\u0e01\u0e34\u0e14']\nb=word_tokenize(text,engine='dict') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nc=word_tokenize(text,engine='mm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nd=word_tokenize(text,engine='pylexto') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\ne=word_tokenize(text,engine='newmm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\n\n\n\n\nThai postaggers\n\n\nfrom pythainlp.tag import pos_tag\npos_tag(list,engine='old')\n\n\n\n\nengine\n\n\n\n\nold is the UnigramTagger (default)\n\n\nartagger is the RDR POS Tagger.\n\n\n\n\nThai romanization\n\n\nfrom pythainlp.romanization import romanization\nromanization(str,engine='pyicu')\n\n\n\n\nThere are 2 engines\n\n\n\n\npyicu\n\n\nroyin\n\n\n\n\ndata :\n\n\ninput ''str''\n\n\nreturns ''str'' \n\n\nExample\n\n\nfrom pythainlp.romanization import romanization\nromanization(\"\u0e41\u0e21\u0e27\") # 'm\u00e6w'\n\n\n\n\nSpell Check\n\n\nBefore using this module,  please install hunspell and hunspell-th.\n\n\nfrom pythainlp.spell import *\na=spell(\"\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e22\u0e21\")\nprint(a) # ['\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2a\u0e35\u0e22\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21']\n\n\n\n\npythainlp.number\n\n\nfrom pythainlp.number import *\n\n\n\n\n\n\nnttn(str)  - convert thai numbers to numbers.\n\n\nnttt(str) - Thai Numbers to text.\n\n\nntnt(str) - numbers to thai numbers.\n\n\nntt(str) -  numbers to text.\n\n\nttn(str) - text to  numbers.\n\n\nnumtowords(float) -  Read thai numbers (Baht) input ''float'' returns  'str'\n\n\n\n\nSort Thai text into List\n\n\nfrom pythainlp.collation import collation\nprint(collation(['\u0e44\u0e01\u0e48','\u0e44\u0e02\u0e48','\u0e01','\u0e2e\u0e32'])) # ['\u0e01', '\u0e44\u0e01\u0e48', '\u0e44\u0e02\u0e48', '\u0e2e\u0e32']\n\n\n\n\ninput list \n\n\nreturns list\n\n\nGet current time in Thai\n\n\nfrom pythainlp.date import now\nnow() # '30 \u0e1e\u0e24\u0e29\u0e20\u0e32\u0e04\u0e21 2560 18:45:24'\n\n\n\n\nThai WordNet\n\n\nimport\n\n\nfrom pythainlp.corpus import wordnet\n\n\n\n\nUse\n\n\nIt's like nltk.\n\n\n\n\nwordnet.synsets(word)\n\n\nwordnet.synset(name_synsets)\n\n\nwordnet.all_lemma_names(pos=None, lang=\"tha\")\n\n\nwordnet.all_synsets(pos=None)\n\n\nwordnet.langs()\n\n\nwordnet.lemmas(word,pos=None,lang=\"tha\")\n\n\nwordnet.lemma(name_synsets)\n\n\nwordnet.lemma_from_key(key)\n\n\nwordnet.path_similarity(synsets1,synsets2)\n\n\nwordnet.lch_similarity(synsets1,synsets2)\n\n\nwordnet.wup_similarity(synsets1,synsets2)\n\n\nwordnet.morphy(form, pos=None)\n\n\nwordnet.custom_lemmas(tab_file, lang)\n\n\n\n\nExample\n\n\n>>> from pythainlp.corpus import wordnet\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07'))\n[Synset('one.s.05'), Synset('one.s.04'), Synset('one.s.01'), Synset('one.n.01')]\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07')[0].lemma_names('tha'))\n[]\n>>> print(wordnet.synset('one.s.05'))\nSynset('one.s.05')\n>>> print(wordnet.synset('spy.n.01').lemmas())\n[Lemma('spy.n.01.spy'), Lemma('spy.n.01.undercover_agent')]\n>>> print(wordnet.synset('spy.n.01').lemma_names('tha'))\n['\u0e2a\u0e1b\u0e32\u0e22', '\u0e2a\u0e32\u0e22\u0e25\u0e31\u0e1a']\n\n\n\n\nFind the most frequent words.\n\n\nfrom pythainlp.rank import rank\nrank(list)\n\n\n\n\nreturns dict\n\n\nExample\n\n\n>>> rank(['\u0e41\u0e21\u0e07','\u0e41\u0e21\u0e07','\u0e04\u0e19'])\nCounter({'\u0e41\u0e21\u0e07': 2, '\u0e04\u0e19': 1})\n\n\n\n\nIncorrect input language correction\n\n\nfrom pythainlp.change import *\n\n\n\n\n\n\ntexttothai(str) - eng to thai.\n\n\ntexttoeng(str) - thai to eng.\n\n\n\n\nThai Character Clusters (TCC)\n\n\nTCC : Mr.Jakkrit TeCho\n\n\ngrammar :  Wittawat Jitkrittum (https://github.com/wittawatj/jtcc/blob/master/TCC.g)\n\n\nCode :  Korakot Chaovavanich \n\n\nExample\n\n\n>>> from pythainlp.tokenize import tcc\n>>> tcc.tcc('\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22')\n'\u0e1b/\u0e23\u0e30/\u0e40\u0e17/\u0e28/\u0e44\u0e17/\u0e22'\n\n\n\n\nEnhanced Thai Character Cluster (ETCC)\n\n\nExample\n\n\n>>> from pythainlp.tokenize import etcc\n>>> etcc.etcc('\u0e04\u0e37\u0e19\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02')\n'/\u0e04\u0e37\u0e19/\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02'\n\n\n\n\nThai Soundex\n\n\ncredit Korakot Chaovavanich (from https://gist.github.com/korakot/0b772e09340cac2f493868da035597e8)\n\n\n\n\nLK82\n\n\nUdom83\n\n\n\n\nExample\n\n\n>>> from pythainlp.soundex import LK82\n>>> print(LK82('\u0e23\u0e16'))\n\u0e233000\n>>> print(LK82('\u0e23\u0e14'))\n\u0e233000\n>>> print(LK82('\u0e08\u0e31\u0e19'))\n\u0e084000\n>>> print(LK82('\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c'))\n\u0e084000\n>>> print(Udom83('\u0e23\u0e16'))\n\u0e23800000\n\n\n\n\nThai meta sound\n\n\nSnae & Br\u00fcckner. (2009). Novel Phonetic Name Matching Algorithm with a Statistical Ontology for Analysing Names Given in Accordance with Thai Astrology. Retrieved from https://pdfs.semanticscholar.org/3983/963e87ddc6dfdbb291099aa3927a0e3e4ea6.pdf\n\n\n\n\nExample\n\n\n>>> from pythainlp.MetaSound import *\n>>> MetaSound('\u0e04\u0e19')\n'15'\n\n\n\n\nThai sentiment analysis\n\n\nusing data from \nhttps://github.com/wannaphongcom/lexicon-thai/tree/master/\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21/\n\n\nfrom pythainlp.sentiment import sentiment\nsentiment(str)\n\n\n\n\ninput str returns pos , neg or neutral\n\n\nUtil\n\n\nusing\n\n\nfrom pythainlp.util import *\n\n\n\n\nngrams\n\n\nfor building ngrams \n\n\nngrams(token,num)\n\n\n\n\n\n\ntoken - list\n\n\nnum - ngrams\n\n\n\n\nCorpus\n\n\nThai stopword\n\n\nfrom pythainlp.corpus import stopwords\nstopwords = stopwords.words('thai')\n\n\n\n\nThai country name\n\n\nfrom pythainlp.corpus import country\ncountry.get_data()\n\n\n\n\nTone in Thai\n\n\nfrom pythainlp.corpus import tone\ntone.get_data()\n\n\n\n\nConsonant in thai\n\n\nfrom pythainlp.corpus import alphabet\nalphabet.get_data()\n\n\n\n\nWord list in thai\n\n\nfrom pythainlp.corpus.thaiword import get_data # old data\nget_data()\nfrom pythainlp.corpus.newthaiword import get_data # new data\nget_data()",
            "title": "English"
        },
        {
            "location": "/pythainlp-1-4-eng/#user-manual-pythainlp-14",
            "text": "User manual PyThaiNLP 1.4  API  Thai segment  Thai postaggers  Thai romanization  Spell Check  pythainlp.number  Sort Thai text into List  Get current time in Thai  Thai WordNet  Find the most frequent words.  Incorrect input language correction  Thai Character Clusters (TCC)  Enhanced Thai Character Cluster (ETCC)  Thai Soundex  Thai meta sound  Thai sentiment analysis  Util  ngrams    Corpus  Thai stopword  Thai country name  Tone in Thai  Consonant in thai  Word list in thai",
            "title": "User manual PyThaiNLP 1.4"
        },
        {
            "location": "/pythainlp-1-4-eng/#api",
            "text": "",
            "title": "API"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-segment",
            "text": "from pythainlp.tokenize import word_tokenize\nword_tokenize(text,engine)  text  refers to an input text string in Thai.  engine  refers to a thai word segmentation system; There are 6 systems to choose from.   icu (default) - pyicu has a very poor performance.   dict - dictionary-based tokenizer. It returns False if the message can not be wrapped.  mm - Maximum Matching algorithm for Thai word segmentation.  newmm - Maximum Matching algorithm for Thai word segmatation. Developed by Korakot Chaovavanich (https://www.facebook.com/groups/408004796247683/permalink/431283740586455/)  pylexto - LexTo.  deepcut - Deep Learning based Thai word segmentation (https://github.com/rkcosmos/deepcut)   Output: ''list'' ex. ['\u0e41\u0e21\u0e27','\u0e01\u0e34\u0e19']  Example  from pythainlp.tokenize import word_tokenize\ntext='\u0e1c\u0e21\u0e23\u0e31\u0e01\u0e04\u0e38\u0e13\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a\u0e42\u0e2d\u0e40\u0e04\u0e1a\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19\u0e44\u0e17\u0e22\u0e23\u0e31\u0e01\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e20\u0e32\u0e29\u0e32\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14'\na=word_tokenize(text,engine='icu') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d', '\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01', '\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19', '\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32', '\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19', '\u0e40\u0e01\u0e34\u0e14']\nb=word_tokenize(text,engine='dict') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nc=word_tokenize(text,engine='mm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nd=word_tokenize(text,engine='pylexto') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\ne=word_tokenize(text,engine='newmm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']",
            "title": "Thai segment"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-postaggers",
            "text": "from pythainlp.tag import pos_tag\npos_tag(list,engine='old')  engine   old is the UnigramTagger (default)  artagger is the RDR POS Tagger.",
            "title": "Thai postaggers"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-romanization",
            "text": "from pythainlp.romanization import romanization\nromanization(str,engine='pyicu')  There are 2 engines   pyicu  royin   data :  input ''str''  returns ''str''   Example  from pythainlp.romanization import romanization\nromanization(\"\u0e41\u0e21\u0e27\") # 'm\u00e6w'",
            "title": "Thai romanization"
        },
        {
            "location": "/pythainlp-1-4-eng/#spell-check",
            "text": "Before using this module,  please install hunspell and hunspell-th.  from pythainlp.spell import *\na=spell(\"\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e22\u0e21\")\nprint(a) # ['\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2a\u0e35\u0e22\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21']",
            "title": "Spell Check"
        },
        {
            "location": "/pythainlp-1-4-eng/#pythainlpnumber",
            "text": "from pythainlp.number import *   nttn(str)  - convert thai numbers to numbers.  nttt(str) - Thai Numbers to text.  ntnt(str) - numbers to thai numbers.  ntt(str) -  numbers to text.  ttn(str) - text to  numbers.  numtowords(float) -  Read thai numbers (Baht) input ''float'' returns  'str'",
            "title": "pythainlp.number"
        },
        {
            "location": "/pythainlp-1-4-eng/#sort-thai-text-into-list",
            "text": "from pythainlp.collation import collation\nprint(collation(['\u0e44\u0e01\u0e48','\u0e44\u0e02\u0e48','\u0e01','\u0e2e\u0e32'])) # ['\u0e01', '\u0e44\u0e01\u0e48', '\u0e44\u0e02\u0e48', '\u0e2e\u0e32']  input list   returns list",
            "title": "Sort Thai text into List"
        },
        {
            "location": "/pythainlp-1-4-eng/#get-current-time-in-thai",
            "text": "from pythainlp.date import now\nnow() # '30 \u0e1e\u0e24\u0e29\u0e20\u0e32\u0e04\u0e21 2560 18:45:24'",
            "title": "Get current time in Thai"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-wordnet",
            "text": "import  from pythainlp.corpus import wordnet  Use  It's like nltk.   wordnet.synsets(word)  wordnet.synset(name_synsets)  wordnet.all_lemma_names(pos=None, lang=\"tha\")  wordnet.all_synsets(pos=None)  wordnet.langs()  wordnet.lemmas(word,pos=None,lang=\"tha\")  wordnet.lemma(name_synsets)  wordnet.lemma_from_key(key)  wordnet.path_similarity(synsets1,synsets2)  wordnet.lch_similarity(synsets1,synsets2)  wordnet.wup_similarity(synsets1,synsets2)  wordnet.morphy(form, pos=None)  wordnet.custom_lemmas(tab_file, lang)   Example  >>> from pythainlp.corpus import wordnet\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07'))\n[Synset('one.s.05'), Synset('one.s.04'), Synset('one.s.01'), Synset('one.n.01')]\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07')[0].lemma_names('tha'))\n[]\n>>> print(wordnet.synset('one.s.05'))\nSynset('one.s.05')\n>>> print(wordnet.synset('spy.n.01').lemmas())\n[Lemma('spy.n.01.spy'), Lemma('spy.n.01.undercover_agent')]\n>>> print(wordnet.synset('spy.n.01').lemma_names('tha'))\n['\u0e2a\u0e1b\u0e32\u0e22', '\u0e2a\u0e32\u0e22\u0e25\u0e31\u0e1a']",
            "title": "Thai WordNet"
        },
        {
            "location": "/pythainlp-1-4-eng/#find-the-most-frequent-words",
            "text": "from pythainlp.rank import rank\nrank(list)  returns dict  Example  >>> rank(['\u0e41\u0e21\u0e07','\u0e41\u0e21\u0e07','\u0e04\u0e19'])\nCounter({'\u0e41\u0e21\u0e07': 2, '\u0e04\u0e19': 1})",
            "title": "Find the most frequent words."
        },
        {
            "location": "/pythainlp-1-4-eng/#incorrect-input-language-correction",
            "text": "from pythainlp.change import *   texttothai(str) - eng to thai.  texttoeng(str) - thai to eng.",
            "title": "Incorrect input language correction"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-character-clusters-tcc",
            "text": "TCC : Mr.Jakkrit TeCho  grammar :  Wittawat Jitkrittum (https://github.com/wittawatj/jtcc/blob/master/TCC.g)  Code :  Korakot Chaovavanich   Example  >>> from pythainlp.tokenize import tcc\n>>> tcc.tcc('\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22')\n'\u0e1b/\u0e23\u0e30/\u0e40\u0e17/\u0e28/\u0e44\u0e17/\u0e22'",
            "title": "Thai Character Clusters (TCC)"
        },
        {
            "location": "/pythainlp-1-4-eng/#enhanced-thai-character-cluster-etcc",
            "text": "Example  >>> from pythainlp.tokenize import etcc\n>>> etcc.etcc('\u0e04\u0e37\u0e19\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02')\n'/\u0e04\u0e37\u0e19/\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02'",
            "title": "Enhanced Thai Character Cluster (ETCC)"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-soundex",
            "text": "credit Korakot Chaovavanich (from https://gist.github.com/korakot/0b772e09340cac2f493868da035597e8)   LK82  Udom83   Example  >>> from pythainlp.soundex import LK82\n>>> print(LK82('\u0e23\u0e16'))\n\u0e233000\n>>> print(LK82('\u0e23\u0e14'))\n\u0e233000\n>>> print(LK82('\u0e08\u0e31\u0e19'))\n\u0e084000\n>>> print(LK82('\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c'))\n\u0e084000\n>>> print(Udom83('\u0e23\u0e16'))\n\u0e23800000",
            "title": "Thai Soundex"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-meta-sound",
            "text": "Snae & Br\u00fcckner. (2009). Novel Phonetic Name Matching Algorithm with a Statistical Ontology for Analysing Names Given in Accordance with Thai Astrology. Retrieved from https://pdfs.semanticscholar.org/3983/963e87ddc6dfdbb291099aa3927a0e3e4ea6.pdf  Example  >>> from pythainlp.MetaSound import *\n>>> MetaSound('\u0e04\u0e19')\n'15'",
            "title": "Thai meta sound"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-sentiment-analysis",
            "text": "using data from  https://github.com/wannaphongcom/lexicon-thai/tree/master/\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21/  from pythainlp.sentiment import sentiment\nsentiment(str)  input str returns pos , neg or neutral",
            "title": "Thai sentiment analysis"
        },
        {
            "location": "/pythainlp-1-4-eng/#util",
            "text": "using  from pythainlp.util import *",
            "title": "Util"
        },
        {
            "location": "/pythainlp-1-4-eng/#ngrams",
            "text": "for building ngrams   ngrams(token,num)   token - list  num - ngrams",
            "title": "ngrams"
        },
        {
            "location": "/pythainlp-1-4-eng/#corpus",
            "text": "",
            "title": "Corpus"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-stopword",
            "text": "from pythainlp.corpus import stopwords\nstopwords = stopwords.words('thai')",
            "title": "Thai stopword"
        },
        {
            "location": "/pythainlp-1-4-eng/#thai-country-name",
            "text": "from pythainlp.corpus import country\ncountry.get_data()",
            "title": "Thai country name"
        },
        {
            "location": "/pythainlp-1-4-eng/#tone-in-thai",
            "text": "from pythainlp.corpus import tone\ntone.get_data()",
            "title": "Tone in Thai"
        },
        {
            "location": "/pythainlp-1-4-eng/#consonant-in-thai",
            "text": "from pythainlp.corpus import alphabet\nalphabet.get_data()",
            "title": "Consonant in thai"
        },
        {
            "location": "/pythainlp-1-4-eng/#word-list-in-thai",
            "text": "from pythainlp.corpus.thaiword import get_data # old data\nget_data()\nfrom pythainlp.corpus.newthaiword import get_data # new data\nget_data()",
            "title": "Word list in thai"
        },
        {
            "location": "/pythainlp-1-4-thai/",
            "text": "\u0e04\u0e39\u0e48\u0e21\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 PyThaiNLP 1.4\n\n\n\n\n\n\n\u0e04\u0e39\u0e48\u0e21\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 PyThaiNLP 1.4\n\n\nAPI\n\n\n\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22\n\n\nPostaggers \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e41\u0e1b\u0e25\u0e07\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19 Latin\n\n\n\u0e40\u0e0a\u0e47\u0e04\u0e04\u0e33\u0e1c\u0e34\u0e14\n\n\npythainlp.number\n\n\n\u0e40\u0e23\u0e35\u0e22\u0e07\u0e25\u0e33\u0e14\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e43\u0e19 List\n\n\n\u0e23\u0e31\u0e1a\u0e40\u0e27\u0e25\u0e32\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nWordNet \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e2b\u0e32\u0e04\u0e33\u0e17\u0e35\u0e48\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\n\n\n\u0e41\u0e01\u0e49\u0e44\u0e02\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e01\u0e32\u0e23\u0e1e\u0e34\u0e21\u0e1e\u0e4c\u0e25\u0e37\u0e21\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e20\u0e32\u0e29\u0e32\n\n\nThai Character Clusters (TCC)\n\n\nEnhanced Thai Character Cluster (ETCC)\n\n\nThai Soundex \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nMeta Sound \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nSentiment analysis \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nUtil\n\n\nngrams\n\n\n\n\n\n\nCorpus\n\n\nstopword \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e0a\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28 \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e15\u0e31\u0e27\u0e27\u0e23\u0e23\u0e13\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e15\u0e31\u0e27\u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e23\u0e32\u0e22\u0e01\u0e32\u0e23\u0e04\u0e33\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural language processing \u0e2b\u0e23\u0e37\u0e2d \u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e20\u0e32\u0e29\u0e32\u0e18\u0e23\u0e23\u0e21\u0e0a\u0e32\u0e15\u0e34  \u0e42\u0e21\u0e14\u0e39\u0e25 PyThaiNLP \u0e40\u0e1b\u0e47\u0e19\u0e42\u0e21\u0e14\u0e39\u0e25\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e02\u0e36\u0e49\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e20\u0e32\u0e29\u0e32\u0e18\u0e23\u0e23\u0e21\u0e0a\u0e32\u0e15\u0e34\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32 Python \u0e41\u0e25\u0e30\n\u0e21\u0e31\u0e19\u0e1f\u0e23\u0e35 (\u0e15\u0e25\u0e2d\u0e14\u0e44\u0e1b) \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e04\u0e19\u0e44\u0e17\u0e22\u0e41\u0e25\u0e30\u0e0a\u0e32\u0e27\u0e42\u0e25\u0e01\u0e17\u0e38\u0e01\u0e04\u0e19 !\n\n\n\n\n\u0e40\u0e1e\u0e23\u0e32\u0e30\u0e42\u0e25\u0e01\u0e02\u0e31\u0e1a\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22\u0e01\u0e32\u0e23\u0e41\u0e1a\u0e48\u0e07\u0e1b\u0e31\u0e19\n\n\n\n\n\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e40\u0e09\u0e1e\u0e32\u0e30 Python 3.4 \u0e02\u0e36\u0e49\u0e19\u0e44\u0e1b\u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19\n\n\n\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\n\n\npip install pythainlp\n\n\n\n\n\u0e27\u0e34\u0e18\u0e35\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a Windows\n\n\n\u0e43\u0e2b\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07 pyicu \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e44\u0e1f\u0e25\u0e4c .whl \u0e08\u0e32\u0e01 \nhttp://www.lfd.uci.edu/~gohlke/pythonlibs/#pyicu\n \n\n\n\u0e2b\u0e32\u0e01\u0e43\u0e0a\u0e49 python 3.5 64 bit \u0e43\u0e2b\u0e49\u0e42\u0e2b\u0e25\u0e14 PyICU\u20111.9.7\u2011cp35\u2011cp35m\u2011win_amd64.whl \u0e41\u0e25\u0e49\u0e27\u0e40\u0e1b\u0e34\u0e14 cmd \u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\n\n\npip install PyICU\u20111.9.7\u2011cp35\u2011cp35m\u2011win_amd64.whl\n\n\n\n\n\u0e41\u0e25\u0e49\u0e27\u0e08\u0e36\u0e07\u0e43\u0e0a\u0e49 \n\n\npip install pythainlp\n\n\n\n\n\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e1a\u0e19 Mac\n\n\n$ brew install icu4c --force\n$ brew link --force icu4c\n$ CFLAGS=-I/usr/local/opt/icu4c/include LDFLAGS=-L/usr/local/opt/icu4c/lib pip install pythainlp\n\n\n\n\n\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21 \n\u0e04\u0e25\u0e34\u0e01\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49\n\n\nAPI\n\n\n\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22\n\n\n\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22\u0e19\u0e31\u0e49\u0e19 \u0e43\u0e0a\u0e49 API \u0e14\u0e31\u0e07\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49\n\n\nfrom pythainlp.tokenize import word_tokenize\nword_tokenize(text,engine)\n\n\n\n\ntext \u0e04\u0e37\u0e2d \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e2a\u0e15\u0e23\u0e34\u0e07 str \u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19\n\n\nengine \u0e04\u0e37\u0e2d \u0e23\u0e30\u0e1a\u0e1a\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22 \u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19\u0e19\u0e35\u0e49 PyThaiNLP \u0e44\u0e14\u0e49\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e21\u0e35 6 engine \u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e01\u0e31\u0e19\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n\n\n\nicu -  engine \u0e15\u0e31\u0e27\u0e14\u0e31\u0e49\u0e07\u0e40\u0e14\u0e34\u0e21\u0e02\u0e2d\u0e07 PyThaiNLP (\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e15\u0e48\u0e33) \u0e41\u0e25\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e48\u0e32\u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19\n\n\ndict - \u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e1e\u0e08\u0e32\u0e19\u0e38\u0e01\u0e23\u0e21\u0e08\u0e32\u0e01 thaiword.txt \u0e43\u0e19 corpus  (\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e1b\u0e32\u0e19\u0e01\u0e25\u0e32\u0e07) \u0e08\u0e30\u0e04\u0e37\u0e19\u0e04\u0e48\u0e32 False \u0e2b\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e19\u0e31\u0e49\u0e19\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e14\u0e49\n\n\nmm - \u0e43\u0e0a\u0e49 Maximum Matching algorithm \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 - API \u0e0a\u0e38\u0e14\u0e40\u0e01\u0e48\u0e32\n\n\nnewmm - \u0e43\u0e0a\u0e49 Maximum Matching algorithm \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 \u0e42\u0e04\u0e49\u0e14\u0e0a\u0e38\u0e14\u0e43\u0e2b\u0e21\u0e48 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e42\u0e04\u0e49\u0e14\u0e04\u0e38\u0e13 Korakot Chaovavanich  \u0e08\u0e32\u0e01 https://www.facebook.com/groups/408004796247683/permalink/431283740586455/ \u0e21\u0e32\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e15\u0e48\u0e2d\n\n\npylexto \u0e43\u0e0a\u0e49 LexTo \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\n\n\ndeepcut \u0e43\u0e0a\u0e49 deepcut \u0e08\u0e32\u0e01 https://github.com/rkcosmos/deepcut \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\n\n\u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''list'' \u0e40\u0e0a\u0e48\u0e19 ['\u0e41\u0e21\u0e27','\u0e01\u0e34\u0e19']\n\n\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\n\n\nfrom pythainlp.tokenize import word_tokenize\ntext='\u0e1c\u0e21\u0e23\u0e31\u0e01\u0e04\u0e38\u0e13\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a\u0e42\u0e2d\u0e40\u0e04\u0e1a\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19\u0e44\u0e17\u0e22\u0e23\u0e31\u0e01\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e20\u0e32\u0e29\u0e32\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14'\na=word_tokenize(text,engine='icu') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d', '\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01', '\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19', '\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32', '\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19', '\u0e40\u0e01\u0e34\u0e14']\nb=word_tokenize(text,engine='dict') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nc=word_tokenize(text,engine='mm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nd=word_tokenize(text,engine='pylexto') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\ne=word_tokenize(text,engine='newmm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\n\n\n\n\nPostaggers \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.tag import pos_tag\npos_tag(list,engine='old')\n\n\n\n\nlist \u0e04\u0e37\u0e2d list \u0e17\u0e35\u0e48\u0e40\u0e01\u0e47\u0e1a\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e2b\u0e25\u0e31\u0e07\u0e1c\u0e48\u0e32\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e41\u0e25\u0e49\u0e27\n\n\nengine \u0e04\u0e37\u0e2d \u0e0a\u0e38\u0e14\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e21\u0e37\u0e2d\u0e43\u0e19\u0e01\u0e32\u0e23 postaggers \u0e21\u0e35 2 \u0e15\u0e31\u0e27\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n\n\n\nold \u0e40\u0e1b\u0e47\u0e19 UnigramTagger (\u0e04\u0e48\u0e32\u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19)\n\n\nartagger \u0e40\u0e1b\u0e47\u0e19 RDR POS Tagger \u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14\u0e22\u0e34\u0e48\u0e07\u0e01\u0e27\u0e48\u0e32\u0e40\u0e14\u0e34\u0e21 \u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e40\u0e09\u0e1e\u0e32\u0e30 Python 3 \u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19\n\n\n\n\n\u0e41\u0e1b\u0e25\u0e07\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19 Latin\n\n\nfrom pythainlp.romanization import romanization\nromanization(str,engine='pyicu')\n\n\n\n\n\u0e21\u0e35 2 engine \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n\n\n\npyicu \u0e2a\u0e48\u0e07\u0e04\u0e48\u0e32 Latin\n\n\nroyin \u0e43\u0e0a\u0e49\u0e2b\u0e25\u0e31\u0e01\u0e40\u0e01\u0e13\u0e11\u0e4c\u0e01\u0e32\u0e23\u0e16\u0e2d\u0e14\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e42\u0e23\u0e21\u0e31\u0e19 \u0e09\u0e1a\u0e31\u0e1a\u0e23\u0e32\u0e0a\u0e1a\u0e31\u0e13\u0e11\u0e34\u0e15\u0e22\u0e2a\u0e16\u0e32\u0e19 (\n\u0e2b\u0e32\u0e01\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14 \u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2d\u0e48\u0e32\u0e19 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e15\u0e31\u0e27 royin \u0e44\u0e21\u0e48\u0e21\u0e35\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e25\u0e07\u0e04\u0e33\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e2d\u0e48\u0e32\u0e19\n)\n\n\n\n\ndata :\n\n\n\u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32 ''str'' \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21 \n\n\n\u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''str'' \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\n\n\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\n\n\nfrom pythainlp.romanization import romanization\nromanization(\"\u0e41\u0e21\u0e27\") # 'm\u00e6w'\n\n\n\n\n\u0e40\u0e0a\u0e47\u0e04\u0e04\u0e33\u0e1c\u0e34\u0e14\n\n\n\u0e01\u0e48\u0e2d\u0e19\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e19\u0e35\u0e49 \u0e43\u0e2b\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07 hunspell \u0e41\u0e25\u0e30 hunspell-th \u0e01\u0e48\u0e2d\u0e19\n\n\n\u0e27\u0e34\u0e18\u0e35\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\n \u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e1a\u0e19 Debian , Ubuntu\n\n\nsudo apt-get install hunspell hunspell-th\n\n\n\n\n\u0e1a\u0e19 Mac OS \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e15\u0e32\u0e21\u0e19\u0e35\u0e49 \nhttp://pankdm.github.io/hunspell.html\n\n\n\u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49 pythainlp.spell \u0e15\u0e32\u0e21\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e19\u0e35\u0e49\n\n\nfrom pythainlp.spell import *\na=spell(\"\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e22\u0e21\")\nprint(a) # ['\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2a\u0e35\u0e22\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21']\n\n\n\n\npythainlp.number\n\n\nfrom pythainlp.number import *\n\n\n\n\n\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e01\u0e31\u0e1a\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02 \u0e42\u0e14\u0e22\u0e21\u0e35\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n\n\n\nnttn(str)  - \u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07\u0e40\u0e25\u0e02\u0e44\u0e17\u0e22\u0e2a\u0e39\u0e48\u0e40\u0e25\u0e02\n\n\nnttt(str) - \u0e40\u0e25\u0e02\u0e44\u0e17\u0e22\u0e2a\u0e39\u0e48\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\n\n\nntnt(str) - \u0e40\u0e25\u0e02\u0e2a\u0e39\u0e48\u0e40\u0e25\u0e02\u0e44\u0e17\u0e22\n\n\nntt(str) - \u0e40\u0e25\u0e02\u0e2a\u0e39\u0e48\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\n\n\nttn(str) - \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e39\u0e48\u0e40\u0e25\u0e02\n\n\nnumtowords(float) -  \u0e2d\u0e48\u0e32\u0e19\u0e08\u0e33\u0e19\u0e27\u0e19\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 (\u0e1a\u0e32\u0e17) \u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''float'' \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19  'str'\n\n\n\n\n\u0e40\u0e23\u0e35\u0e22\u0e07\u0e25\u0e33\u0e14\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e43\u0e19 List\n\n\nfrom pythainlp.collation import collation\nprint(collation(['\u0e44\u0e01\u0e48','\u0e44\u0e02\u0e48','\u0e01','\u0e2e\u0e32'])) # ['\u0e01', '\u0e44\u0e01\u0e48', '\u0e44\u0e02\u0e48', '\u0e2e\u0e32']\n\n\n\n\n\u0e23\u0e31\u0e1a list \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32 list\n\n\n\u0e23\u0e31\u0e1a\u0e40\u0e27\u0e25\u0e32\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.date import now\nnow() # '30 \u0e1e\u0e24\u0e29\u0e20\u0e32\u0e04\u0e21 2560 18:45:24'\n\n\n\n\nWordNet \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e40\u0e23\u0e35\u0e22\u0e01\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\nfrom pythainlp.corpus import wordnet\n\n\n\n\n\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\nAPI \u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e1a NLTK \u0e42\u0e14\u0e22\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a API \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n\n\n\nwordnet.synsets(word)\n\n\nwordnet.synset(name_synsets)\n\n\nwordnet.all_lemma_names(pos=None, lang=\"tha\")\n\n\nwordnet.all_synsets(pos=None)\n\n\nwordnet.langs()\n\n\nwordnet.lemmas(word,pos=None,lang=\"tha\")\n\n\nwordnet.lemma(name_synsets)\n\n\nwordnet.lemma_from_key(key)\n\n\nwordnet.path_similarity(synsets1,synsets2)\n\n\nwordnet.lch_similarity(synsets1,synsets2)\n\n\nwordnet.wup_similarity(synsets1,synsets2)\n\n\nwordnet.morphy(form, pos=None)\n\n\nwordnet.custom_lemmas(tab_file, lang)\n\n\n\n\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\n\n\n>>> from pythainlp.corpus import wordnet\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07'))\n[Synset('one.s.05'), Synset('one.s.04'), Synset('one.s.01'), Synset('one.n.01')]\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07')[0].lemma_names('tha'))\n[]\n>>> print(wordnet.synset('one.s.05'))\nSynset('one.s.05')\n>>> print(wordnet.synset('spy.n.01').lemmas())\n[Lemma('spy.n.01.spy'), Lemma('spy.n.01.undercover_agent')]\n>>> print(wordnet.synset('spy.n.01').lemma_names('tha'))\n['\u0e2a\u0e1b\u0e32\u0e22', '\u0e2a\u0e32\u0e22\u0e25\u0e31\u0e1a']\n\n\n\n\n\u0e2b\u0e32\u0e04\u0e33\u0e17\u0e35\u0e48\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14\n\n\nfrom pythainlp.rank import rank\nrank(list)\n\n\n\n\n\u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19 dict\n\n\n\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\n>>> rank(['\u0e41\u0e21\u0e07','\u0e41\u0e21\u0e07','\u0e04\u0e19'])\nCounter({'\u0e41\u0e21\u0e07': 2, '\u0e04\u0e19': 1})\n\n\n\n\n\u0e41\u0e01\u0e49\u0e44\u0e02\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e01\u0e32\u0e23\u0e1e\u0e34\u0e21\u0e1e\u0e4c\u0e25\u0e37\u0e21\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e20\u0e32\u0e29\u0e32\n\n\nfrom pythainlp.change import *\n\n\n\n\n\u0e21\u0e35\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49\n\n\n\n\ntexttothai(str) \u0e41\u0e1b\u0e25\u0e07\u0e41\u0e1b\u0e49\u0e19\u0e15\u0e31\u0e27\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e20\u0e32\u0e29\u0e32\u0e2d\u0e31\u0e07\u0e01\u0e24\u0e29\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\ntexttoeng(str) \u0e41\u0e1b\u0e25\u0e07\u0e41\u0e1b\u0e49\u0e19\u0e15\u0e31\u0e27\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e2d\u0e31\u0e07\u0e01\u0e24\u0e29\n\n\n\n\n\u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19 str\n\n\nThai Character Clusters (TCC)\n\n\nPyThaiNLP 1.4 \u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a Thai Character Clusters (TCC) \u0e42\u0e14\u0e22\u0e08\u0e30\u0e41\u0e1a\u0e48\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e14\u0e49\u0e27\u0e22 /\n\n\n\u0e40\u0e14\u0e15\u0e34\u0e14\n\n\nTCC : Mr.Jakkrit TeCho\n\n\ngrammar : \u0e04\u0e38\u0e13 Wittawat Jitkrittum (https://github.com/wittawatj/jtcc/blob/master/TCC.g)\n\n\n\u0e42\u0e04\u0e49\u0e14 : \u0e04\u0e38\u0e13 Korakot Chaovavanich \n\n\n\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\n>>> from pythainlp.tokenize import tcc\n>>> tcc.tcc('\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22')\n'\u0e1b/\u0e23\u0e30/\u0e40\u0e17/\u0e28/\u0e44\u0e17/\u0e22'\n\n\n\n\nEnhanced Thai Character Cluster (ETCC)\n\n\n\u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01 TCC \u0e41\u0e25\u0e49\u0e27 PyThaiNLP 1.4 \u0e22\u0e31\u0e07\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a Enhanced Thai Character Cluster (ETCC) \u0e42\u0e14\u0e22\u0e41\u0e1a\u0e48\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e14\u0e49\u0e27\u0e22 /\n\n\n\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\n>>> from pythainlp.tokenize import etcc\n>>> etcc.etcc('\u0e04\u0e37\u0e19\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02')\n'/\u0e04\u0e37\u0e19/\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02'\n\n\n\n\nThai Soundex \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e40\u0e14\u0e15\u0e34\u0e14 \u0e04\u0e38\u0e13 Korakot Chaovavanich (\u0e08\u0e32\u0e01 https://gist.github.com/korakot/0b772e09340cac2f493868da035597e8)\n\n\n\u0e01\u0e0e\u0e17\u0e35\u0e48\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e43\u0e19\u0e40\u0e27\u0e0a\u0e31\u0e48\u0e19 1.4\n\n\n\n\n\u0e01\u0e0e\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e23\u0e2b\u0e31\u0e2a\u0e0b\u0e32\u0e27\u0e19\u0e4c\u0e40\u0e14\u0e47\u0e01\u0e0b\u0e4c\u0e02\u0e2d\u0e07  \u0e27\u0e34\u0e0a\u0e34\u0e15\u0e2b\u0e25\u0e48\u0e2d\u0e08\u0e35\u0e23\u0e30\u0e0a\u0e38\u0e13\u0e2b\u0e4c\u0e01\u0e38\u0e25  \u0e41\u0e25\u0e30  \u0e40\u0e08\u0e23\u0e34\u0e0d  \u0e04\u0e38\u0e27\u0e34\u0e19\u0e17\u0e23\u0e4c\u0e1e\u0e31\u0e19\u0e18\u0e38\u0e4c - LK82\n\n\n\u0e01\u0e0e\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e23\u0e2b\u0e31\u0e2a\u0e0b\u0e32\u0e27\u0e19\u0e4c\u0e40\u0e14\u0e47\u0e01\u0e0b\u0e4c\u0e02\u0e2d\u0e07 \u0e27\u0e23\u0e23\u0e13\u0e35 \u0e2d\u0e38\u0e14\u0e21\u0e1e\u0e32\u0e13\u0e34\u0e0a\u0e22\u0e4c - Udom83\n\n\n\n\n\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\n>>> from pythainlp.soundex import LK82\n>>> print(LK82('\u0e23\u0e16'))\n\u0e233000\n>>> print(LK82('\u0e23\u0e14'))\n\u0e233000\n>>> print(LK82('\u0e08\u0e31\u0e19'))\n\u0e084000\n>>> print(LK82('\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c'))\n\u0e084000\n>>> print(Udom83('\u0e23\u0e16'))\n\u0e23800000\n\n\n\n\nMeta Sound \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nSnae & Br\u00fcckner. (2009). Novel Phonetic Name Matching Algorithm with a Statistical Ontology for Analysing Names Given in Accordance with Thai Astrology. Retrieved from https://pdfs.semanticscholar.org/3983/963e87ddc6dfdbb291099aa3927a0e3e4ea6.pdf\n\n\n\n\n\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\n>>> from pythainlp.MetaSound import *\n>>> MetaSound('\u0e04\u0e19')\n'15'\n\n\n\n\nSentiment analysis \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\n\u0e43\u0e0a\u0e49\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01 \nhttps://github.com/wannaphongcom/lexicon-thai/tree/master/\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21/\n\n\nfrom pythainlp.sentiment import sentiment\nsentiment(str)\n\n\n\n\n\u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32 str \u0e2a\u0e48\u0e07\u0e2d\u0e2d\u0e01\u0e40\u0e1b\u0e47\u0e19 pos , neg \u0e2b\u0e23\u0e37\u0e2d neutral\n\n\nUtil\n\n\n\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\n\n\nfrom pythainlp.util import *\n\n\n\n\nngrams\n\n\n\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e2a\u0e23\u0e49\u0e32\u0e07 ngrams \n\n\nngrams(token,num)\n\n\n\n\n\n\ntoken \u0e04\u0e37\u0e2d list\n\n\nnum \u0e04\u0e37\u0e2d \u0e08\u0e33\u0e19\u0e27\u0e19 ngrams\n\n\n\n\nCorpus\n\n\nstopword \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.corpus import stopwords\nstopwords = stopwords.words('thai')\n\n\n\n\n\u0e0a\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28 \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.corpus import country\ncountry.get_data()\n\n\n\n\n\u0e15\u0e31\u0e27\u0e27\u0e23\u0e23\u0e13\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.corpus import tone\ntone.get_data()\n\n\n\n\n\u0e15\u0e31\u0e27\u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.corpus import alphabet\nalphabet.get_data()\n\n\n\n\n\u0e23\u0e32\u0e22\u0e01\u0e32\u0e23\u0e04\u0e33\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\n\n\nfrom pythainlp.corpus.thaiword import get_data # \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e01\u0e48\u0e32\nget_data()\nfrom pythainlp.corpus.newthaiword import get_data # \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e2b\u0e21\u0e48\nget_data()\n\n\n\n\n\u0e40\u0e02\u0e35\u0e22\u0e19\u0e42\u0e14\u0e22 \u0e19\u0e32\u0e22 \u0e27\u0e23\u0e23\u0e13\u0e1e\u0e07\u0e29\u0e4c  \u0e20\u0e31\u0e17\u0e17\u0e34\u0e22\u0e44\u0e1e\u0e1a\u0e39\u0e25\u0e22\u0e4c",
            "title": "Thai"
        },
        {
            "location": "/pythainlp-1-4-thai/#pythainlp-14",
            "text": "\u0e04\u0e39\u0e48\u0e21\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 PyThaiNLP 1.4  API  \u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22  Postaggers \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  \u0e41\u0e1b\u0e25\u0e07\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19 Latin  \u0e40\u0e0a\u0e47\u0e04\u0e04\u0e33\u0e1c\u0e34\u0e14  pythainlp.number  \u0e40\u0e23\u0e35\u0e22\u0e07\u0e25\u0e33\u0e14\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e43\u0e19 List  \u0e23\u0e31\u0e1a\u0e40\u0e27\u0e25\u0e32\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  WordNet \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  \u0e2b\u0e32\u0e04\u0e33\u0e17\u0e35\u0e48\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14  \u0e41\u0e01\u0e49\u0e44\u0e02\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e01\u0e32\u0e23\u0e1e\u0e34\u0e21\u0e1e\u0e4c\u0e25\u0e37\u0e21\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e20\u0e32\u0e29\u0e32  Thai Character Clusters (TCC)  Enhanced Thai Character Cluster (ETCC)  Thai Soundex \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  Meta Sound \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  Sentiment analysis \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  Util  ngrams    Corpus  stopword \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  \u0e0a\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28 \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  \u0e15\u0e31\u0e27\u0e27\u0e23\u0e23\u0e13\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  \u0e15\u0e31\u0e27\u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  \u0e23\u0e32\u0e22\u0e01\u0e32\u0e23\u0e04\u0e33\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22          Natural language processing \u0e2b\u0e23\u0e37\u0e2d \u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e20\u0e32\u0e29\u0e32\u0e18\u0e23\u0e23\u0e21\u0e0a\u0e32\u0e15\u0e34  \u0e42\u0e21\u0e14\u0e39\u0e25 PyThaiNLP \u0e40\u0e1b\u0e47\u0e19\u0e42\u0e21\u0e14\u0e39\u0e25\u0e17\u0e35\u0e48\u0e16\u0e39\u0e01\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e02\u0e36\u0e49\u0e19\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e01\u0e32\u0e23\u0e1b\u0e23\u0e30\u0e21\u0e27\u0e25\u0e20\u0e32\u0e29\u0e32\u0e18\u0e23\u0e23\u0e21\u0e0a\u0e32\u0e15\u0e34\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32 Python \u0e41\u0e25\u0e30 \u0e21\u0e31\u0e19\u0e1f\u0e23\u0e35 (\u0e15\u0e25\u0e2d\u0e14\u0e44\u0e1b) \u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e04\u0e19\u0e44\u0e17\u0e22\u0e41\u0e25\u0e30\u0e0a\u0e32\u0e27\u0e42\u0e25\u0e01\u0e17\u0e38\u0e01\u0e04\u0e19 !   \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e42\u0e25\u0e01\u0e02\u0e31\u0e1a\u0e40\u0e04\u0e25\u0e37\u0e48\u0e2d\u0e19\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e14\u0e49\u0e27\u0e22\u0e01\u0e32\u0e23\u0e41\u0e1a\u0e48\u0e07\u0e1b\u0e31\u0e19   \u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e40\u0e09\u0e1e\u0e32\u0e30 Python 3.4 \u0e02\u0e36\u0e49\u0e19\u0e44\u0e1b\u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19  \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07  pip install pythainlp  \u0e27\u0e34\u0e18\u0e35\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a Windows  \u0e43\u0e2b\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07 pyicu \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e44\u0e1f\u0e25\u0e4c .whl \u0e08\u0e32\u0e01  http://www.lfd.uci.edu/~gohlke/pythonlibs/#pyicu    \u0e2b\u0e32\u0e01\u0e43\u0e0a\u0e49 python 3.5 64 bit \u0e43\u0e2b\u0e49\u0e42\u0e2b\u0e25\u0e14 PyICU\u20111.9.7\u2011cp35\u2011cp35m\u2011win_amd64.whl \u0e41\u0e25\u0e49\u0e27\u0e40\u0e1b\u0e34\u0e14 cmd \u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07  pip install PyICU\u20111.9.7\u2011cp35\u2011cp35m\u2011win_amd64.whl  \u0e41\u0e25\u0e49\u0e27\u0e08\u0e36\u0e07\u0e43\u0e0a\u0e49   pip install pythainlp  \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e1a\u0e19 Mac  $ brew install icu4c --force\n$ brew link --force icu4c\n$ CFLAGS=-I/usr/local/opt/icu4c/include LDFLAGS=-L/usr/local/opt/icu4c/lib pip install pythainlp  \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1e\u0e34\u0e48\u0e21\u0e40\u0e15\u0e34\u0e21  \u0e04\u0e25\u0e34\u0e01\u0e17\u0e35\u0e48\u0e19\u0e35\u0e49",
            "title": "\u0e04\u0e39\u0e48\u0e21\u0e37\u0e2d\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19 PyThaiNLP 1.4"
        },
        {
            "location": "/pythainlp-1-4-thai/#api",
            "text": "",
            "title": "API"
        },
        {
            "location": "/pythainlp-1-4-thai/#_1",
            "text": "\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22\u0e19\u0e31\u0e49\u0e19 \u0e43\u0e0a\u0e49 API \u0e14\u0e31\u0e07\u0e15\u0e48\u0e2d\u0e44\u0e1b\u0e19\u0e35\u0e49  from pythainlp.tokenize import word_tokenize\nword_tokenize(text,engine)  text \u0e04\u0e37\u0e2d \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e43\u0e19\u0e23\u0e39\u0e1b\u0e41\u0e1a\u0e1a\u0e2a\u0e15\u0e23\u0e34\u0e07 str \u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19  engine \u0e04\u0e37\u0e2d \u0e23\u0e30\u0e1a\u0e1a\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22 \u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19\u0e19\u0e35\u0e49 PyThaiNLP \u0e44\u0e14\u0e49\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e21\u0e35 6 engine \u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e01\u0e31\u0e19\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49   icu -  engine \u0e15\u0e31\u0e27\u0e14\u0e31\u0e49\u0e07\u0e40\u0e14\u0e34\u0e21\u0e02\u0e2d\u0e07 PyThaiNLP (\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e15\u0e48\u0e33) \u0e41\u0e25\u0e30\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e48\u0e32\u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19  dict - \u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e1e\u0e08\u0e32\u0e19\u0e38\u0e01\u0e23\u0e21\u0e08\u0e32\u0e01 thaiword.txt \u0e43\u0e19 corpus  (\u0e04\u0e27\u0e32\u0e21\u0e41\u0e21\u0e48\u0e19\u0e22\u0e33\u0e1b\u0e32\u0e19\u0e01\u0e25\u0e32\u0e07) \u0e08\u0e30\u0e04\u0e37\u0e19\u0e04\u0e48\u0e32 False \u0e2b\u0e32\u0e01\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e19\u0e31\u0e49\u0e19\u0e44\u0e21\u0e48\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e14\u0e49  mm - \u0e43\u0e0a\u0e49 Maximum Matching algorithm \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 - API \u0e0a\u0e38\u0e14\u0e40\u0e01\u0e48\u0e32  newmm - \u0e43\u0e0a\u0e49 Maximum Matching algorithm \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 \u0e42\u0e04\u0e49\u0e14\u0e0a\u0e38\u0e14\u0e43\u0e2b\u0e21\u0e48 \u0e42\u0e14\u0e22\u0e43\u0e0a\u0e49\u0e42\u0e04\u0e49\u0e14\u0e04\u0e38\u0e13 Korakot Chaovavanich  \u0e08\u0e32\u0e01 https://www.facebook.com/groups/408004796247683/permalink/431283740586455/ \u0e21\u0e32\u0e1e\u0e31\u0e12\u0e19\u0e32\u0e15\u0e48\u0e2d  pylexto \u0e43\u0e0a\u0e49 LexTo \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33  deepcut \u0e43\u0e0a\u0e49 deepcut \u0e08\u0e32\u0e01 https://github.com/rkcosmos/deepcut \u0e43\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22   \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''list'' \u0e40\u0e0a\u0e48\u0e19 ['\u0e41\u0e21\u0e27','\u0e01\u0e34\u0e19']  \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07  from pythainlp.tokenize import word_tokenize\ntext='\u0e1c\u0e21\u0e23\u0e31\u0e01\u0e04\u0e38\u0e13\u0e19\u0e30\u0e04\u0e23\u0e31\u0e1a\u0e42\u0e2d\u0e40\u0e04\u0e1a\u0e48\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e19\u0e44\u0e17\u0e22\u0e23\u0e31\u0e01\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e20\u0e32\u0e29\u0e32\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14'\na=word_tokenize(text,engine='icu') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d', '\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01', '\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19', '\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32', '\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19', '\u0e40\u0e01\u0e34\u0e14']\nb=word_tokenize(text,engine='dict') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nc=word_tokenize(text,engine='mm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\nd=word_tokenize(text,engine='pylexto') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']\ne=word_tokenize(text,engine='newmm') # ['\u0e1c\u0e21', '\u0e23\u0e31\u0e01', '\u0e04\u0e38\u0e13', '\u0e19\u0e30', '\u0e04\u0e23\u0e31\u0e1a', '\u0e42\u0e2d\u0e40\u0e04', '\u0e1a\u0e48', '\u0e1e\u0e27\u0e01\u0e40\u0e23\u0e32', '\u0e40\u0e1b\u0e47\u0e19', '\u0e04\u0e19\u0e44\u0e17\u0e22', '\u0e23\u0e31\u0e01', '\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22', '\u0e20\u0e32\u0e29\u0e32', '\u0e1a\u0e49\u0e32\u0e19\u0e40\u0e01\u0e34\u0e14']",
            "title": "\u0e15\u0e31\u0e14\u0e04\u0e33\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#postaggers",
            "text": "from pythainlp.tag import pos_tag\npos_tag(list,engine='old')  list \u0e04\u0e37\u0e2d list \u0e17\u0e35\u0e48\u0e40\u0e01\u0e47\u0e1a\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e2b\u0e25\u0e31\u0e07\u0e1c\u0e48\u0e32\u0e19\u0e01\u0e32\u0e23\u0e15\u0e31\u0e14\u0e04\u0e33\u0e41\u0e25\u0e49\u0e27  engine \u0e04\u0e37\u0e2d \u0e0a\u0e38\u0e14\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e21\u0e37\u0e2d\u0e43\u0e19\u0e01\u0e32\u0e23 postaggers \u0e21\u0e35 2 \u0e15\u0e31\u0e27\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49   old \u0e40\u0e1b\u0e47\u0e19 UnigramTagger (\u0e04\u0e48\u0e32\u0e40\u0e23\u0e34\u0e48\u0e21\u0e15\u0e49\u0e19)  artagger \u0e40\u0e1b\u0e47\u0e19 RDR POS Tagger \u0e25\u0e30\u0e40\u0e2d\u0e35\u0e22\u0e14\u0e22\u0e34\u0e48\u0e07\u0e01\u0e27\u0e48\u0e32\u0e40\u0e14\u0e34\u0e21 \u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e40\u0e09\u0e1e\u0e32\u0e30 Python 3 \u0e40\u0e17\u0e48\u0e32\u0e19\u0e31\u0e49\u0e19",
            "title": "Postaggers \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#latin",
            "text": "from pythainlp.romanization import romanization\nromanization(str,engine='pyicu')  \u0e21\u0e35 2 engine \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49   pyicu \u0e2a\u0e48\u0e07\u0e04\u0e48\u0e32 Latin  royin \u0e43\u0e0a\u0e49\u0e2b\u0e25\u0e31\u0e01\u0e40\u0e01\u0e13\u0e11\u0e4c\u0e01\u0e32\u0e23\u0e16\u0e2d\u0e14\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e42\u0e23\u0e21\u0e31\u0e19 \u0e09\u0e1a\u0e31\u0e1a\u0e23\u0e32\u0e0a\u0e1a\u0e31\u0e13\u0e11\u0e34\u0e15\u0e22\u0e2a\u0e16\u0e32\u0e19 ( \u0e2b\u0e32\u0e01\u0e21\u0e35\u0e02\u0e49\u0e2d\u0e1c\u0e34\u0e14\u0e1e\u0e25\u0e32\u0e14 \u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49\u0e04\u0e33\u0e2d\u0e48\u0e32\u0e19 \u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e08\u0e32\u0e01\u0e15\u0e31\u0e27 royin \u0e44\u0e21\u0e48\u0e21\u0e35\u0e15\u0e31\u0e27\u0e41\u0e1b\u0e25\u0e07\u0e04\u0e33\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e33\u0e2d\u0e48\u0e32\u0e19 )   data :  \u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32 ''str'' \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21   \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''str'' \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21  \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07  from pythainlp.romanization import romanization\nromanization(\"\u0e41\u0e21\u0e27\") # 'm\u00e6w'",
            "title": "\u0e41\u0e1b\u0e25\u0e07\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e40\u0e1b\u0e47\u0e19 Latin"
        },
        {
            "location": "/pythainlp-1-4-thai/#_2",
            "text": "\u0e01\u0e48\u0e2d\u0e19\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e32\u0e21\u0e32\u0e23\u0e16\u0e19\u0e35\u0e49 \u0e43\u0e2b\u0e49\u0e17\u0e33\u0e01\u0e32\u0e23\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07 hunspell \u0e41\u0e25\u0e30 hunspell-th \u0e01\u0e48\u0e2d\u0e19  \u0e27\u0e34\u0e18\u0e35\u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07  \u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e1a\u0e19 Debian , Ubuntu  sudo apt-get install hunspell hunspell-th  \u0e1a\u0e19 Mac OS \u0e15\u0e34\u0e14\u0e15\u0e31\u0e49\u0e07\u0e15\u0e32\u0e21\u0e19\u0e35\u0e49  http://pankdm.github.io/hunspell.html  \u0e43\u0e2b\u0e49\u0e43\u0e0a\u0e49 pythainlp.spell \u0e15\u0e32\u0e21\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e19\u0e35\u0e49  from pythainlp.spell import *\na=spell(\"\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e22\u0e21\")\nprint(a) # ['\u0e2a\u0e35\u0e48\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2a\u0e35\u0e22\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21', '\u0e40\u0e2b\u0e25\u0e35\u0e48\u0e22\u0e21']",
            "title": "\u0e40\u0e0a\u0e47\u0e04\u0e04\u0e33\u0e1c\u0e34\u0e14"
        },
        {
            "location": "/pythainlp-1-4-thai/#pythainlpnumber",
            "text": "from pythainlp.number import *  \u0e08\u0e31\u0e14\u0e01\u0e32\u0e23\u0e01\u0e31\u0e1a\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02 \u0e42\u0e14\u0e22\u0e21\u0e35\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49   nttn(str)  - \u0e40\u0e1b\u0e47\u0e19\u0e01\u0e32\u0e23\u0e41\u0e1b\u0e25\u0e07\u0e40\u0e25\u0e02\u0e44\u0e17\u0e22\u0e2a\u0e39\u0e48\u0e40\u0e25\u0e02  nttt(str) - \u0e40\u0e25\u0e02\u0e44\u0e17\u0e22\u0e2a\u0e39\u0e48\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21  ntnt(str) - \u0e40\u0e25\u0e02\u0e2a\u0e39\u0e48\u0e40\u0e25\u0e02\u0e44\u0e17\u0e22  ntt(str) - \u0e40\u0e25\u0e02\u0e2a\u0e39\u0e48\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21  ttn(str) - \u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e39\u0e48\u0e40\u0e25\u0e02  numtowords(float) -  \u0e2d\u0e48\u0e32\u0e19\u0e08\u0e33\u0e19\u0e27\u0e19\u0e15\u0e31\u0e27\u0e40\u0e25\u0e02\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22 (\u0e1a\u0e32\u0e17) \u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19 ''float'' \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e40\u0e1b\u0e47\u0e19  'str'",
            "title": "pythainlp.number"
        },
        {
            "location": "/pythainlp-1-4-thai/#list",
            "text": "from pythainlp.collation import collation\nprint(collation(['\u0e44\u0e01\u0e48','\u0e44\u0e02\u0e48','\u0e01','\u0e2e\u0e32'])) # ['\u0e01', '\u0e44\u0e01\u0e48', '\u0e44\u0e02\u0e48', '\u0e2e\u0e32']  \u0e23\u0e31\u0e1a list \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32 list",
            "title": "\u0e40\u0e23\u0e35\u0e22\u0e07\u0e25\u0e33\u0e14\u0e31\u0e1a\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e43\u0e19 List"
        },
        {
            "location": "/pythainlp-1-4-thai/#_3",
            "text": "from pythainlp.date import now\nnow() # '30 \u0e1e\u0e24\u0e29\u0e20\u0e32\u0e04\u0e21 2560 18:45:24'",
            "title": "\u0e23\u0e31\u0e1a\u0e40\u0e27\u0e25\u0e32\u0e1b\u0e31\u0e08\u0e08\u0e38\u0e1a\u0e31\u0e19\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#wordnet",
            "text": "\u0e40\u0e23\u0e35\u0e22\u0e01\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  from pythainlp.corpus import wordnet  \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  API \u0e40\u0e2b\u0e21\u0e37\u0e2d\u0e19\u0e01\u0e31\u0e1a NLTK \u0e42\u0e14\u0e22\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a API \u0e14\u0e31\u0e07\u0e19\u0e35\u0e49   wordnet.synsets(word)  wordnet.synset(name_synsets)  wordnet.all_lemma_names(pos=None, lang=\"tha\")  wordnet.all_synsets(pos=None)  wordnet.langs()  wordnet.lemmas(word,pos=None,lang=\"tha\")  wordnet.lemma(name_synsets)  wordnet.lemma_from_key(key)  wordnet.path_similarity(synsets1,synsets2)  wordnet.lch_similarity(synsets1,synsets2)  wordnet.wup_similarity(synsets1,synsets2)  wordnet.morphy(form, pos=None)  wordnet.custom_lemmas(tab_file, lang)   \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07  >>> from pythainlp.corpus import wordnet\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07'))\n[Synset('one.s.05'), Synset('one.s.04'), Synset('one.s.01'), Synset('one.n.01')]\n>>> print(wordnet.synsets('\u0e2b\u0e19\u0e36\u0e48\u0e07')[0].lemma_names('tha'))\n[]\n>>> print(wordnet.synset('one.s.05'))\nSynset('one.s.05')\n>>> print(wordnet.synset('spy.n.01').lemmas())\n[Lemma('spy.n.01.spy'), Lemma('spy.n.01.undercover_agent')]\n>>> print(wordnet.synset('spy.n.01').lemma_names('tha'))\n['\u0e2a\u0e1b\u0e32\u0e22', '\u0e2a\u0e32\u0e22\u0e25\u0e31\u0e1a']",
            "title": "WordNet \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#_4",
            "text": "from pythainlp.rank import rank\nrank(list)  \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19 dict  \u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  >>> rank(['\u0e41\u0e21\u0e07','\u0e41\u0e21\u0e07','\u0e04\u0e19'])\nCounter({'\u0e41\u0e21\u0e07': 2, '\u0e04\u0e19': 1})",
            "title": "\u0e2b\u0e32\u0e04\u0e33\u0e17\u0e35\u0e48\u0e21\u0e35\u0e08\u0e33\u0e19\u0e27\u0e19\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19\u0e21\u0e32\u0e01\u0e17\u0e35\u0e48\u0e2a\u0e38\u0e14"
        },
        {
            "location": "/pythainlp-1-4-thai/#_5",
            "text": "from pythainlp.change import *  \u0e21\u0e35\u0e04\u0e33\u0e2a\u0e31\u0e48\u0e07\u0e14\u0e31\u0e07\u0e19\u0e35\u0e49   texttothai(str) \u0e41\u0e1b\u0e25\u0e07\u0e41\u0e1b\u0e49\u0e19\u0e15\u0e31\u0e27\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e20\u0e32\u0e29\u0e32\u0e2d\u0e31\u0e07\u0e01\u0e24\u0e29\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22  texttoeng(str) \u0e41\u0e1b\u0e25\u0e07\u0e41\u0e1b\u0e49\u0e19\u0e15\u0e31\u0e27\u0e2d\u0e31\u0e01\u0e29\u0e23\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\u0e40\u0e1b\u0e47\u0e19\u0e20\u0e32\u0e29\u0e32\u0e2d\u0e31\u0e07\u0e01\u0e24\u0e29   \u0e04\u0e37\u0e19\u0e04\u0e48\u0e32\u0e2d\u0e2d\u0e01\u0e21\u0e32\u0e40\u0e1b\u0e47\u0e19 str",
            "title": "\u0e41\u0e01\u0e49\u0e44\u0e02\u0e1b\u0e31\u0e0d\u0e2b\u0e32\u0e01\u0e32\u0e23\u0e1e\u0e34\u0e21\u0e1e\u0e4c\u0e25\u0e37\u0e21\u0e40\u0e1b\u0e25\u0e35\u0e48\u0e22\u0e19\u0e20\u0e32\u0e29\u0e32"
        },
        {
            "location": "/pythainlp-1-4-thai/#thai-character-clusters-tcc",
            "text": "PyThaiNLP 1.4 \u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a Thai Character Clusters (TCC) \u0e42\u0e14\u0e22\u0e08\u0e30\u0e41\u0e1a\u0e48\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e14\u0e49\u0e27\u0e22 /  \u0e40\u0e14\u0e15\u0e34\u0e14  TCC : Mr.Jakkrit TeCho  grammar : \u0e04\u0e38\u0e13 Wittawat Jitkrittum (https://github.com/wittawatj/jtcc/blob/master/TCC.g)  \u0e42\u0e04\u0e49\u0e14 : \u0e04\u0e38\u0e13 Korakot Chaovavanich   \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  >>> from pythainlp.tokenize import tcc\n>>> tcc.tcc('\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28\u0e44\u0e17\u0e22')\n'\u0e1b/\u0e23\u0e30/\u0e40\u0e17/\u0e28/\u0e44\u0e17/\u0e22'",
            "title": "Thai Character Clusters (TCC)"
        },
        {
            "location": "/pythainlp-1-4-thai/#enhanced-thai-character-cluster-etcc",
            "text": "\u0e19\u0e2d\u0e01\u0e08\u0e32\u0e01 TCC \u0e41\u0e25\u0e49\u0e27 PyThaiNLP 1.4 \u0e22\u0e31\u0e07\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a Enhanced Thai Character Cluster (ETCC) \u0e42\u0e14\u0e22\u0e41\u0e1a\u0e48\u0e07\u0e01\u0e25\u0e38\u0e48\u0e21\u0e14\u0e49\u0e27\u0e22 /  \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  >>> from pythainlp.tokenize import etcc\n>>> etcc.etcc('\u0e04\u0e37\u0e19\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02')\n'/\u0e04\u0e37\u0e19/\u0e04\u0e27\u0e32\u0e21\u0e2a\u0e38\u0e02'",
            "title": "Enhanced Thai Character Cluster (ETCC)"
        },
        {
            "location": "/pythainlp-1-4-thai/#thai-soundex",
            "text": "\u0e40\u0e14\u0e15\u0e34\u0e14 \u0e04\u0e38\u0e13 Korakot Chaovavanich (\u0e08\u0e32\u0e01 https://gist.github.com/korakot/0b772e09340cac2f493868da035597e8)  \u0e01\u0e0e\u0e17\u0e35\u0e48\u0e23\u0e2d\u0e07\u0e23\u0e31\u0e1a\u0e43\u0e19\u0e40\u0e27\u0e0a\u0e31\u0e48\u0e19 1.4   \u0e01\u0e0e\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e23\u0e2b\u0e31\u0e2a\u0e0b\u0e32\u0e27\u0e19\u0e4c\u0e40\u0e14\u0e47\u0e01\u0e0b\u0e4c\u0e02\u0e2d\u0e07  \u0e27\u0e34\u0e0a\u0e34\u0e15\u0e2b\u0e25\u0e48\u0e2d\u0e08\u0e35\u0e23\u0e30\u0e0a\u0e38\u0e13\u0e2b\u0e4c\u0e01\u0e38\u0e25  \u0e41\u0e25\u0e30  \u0e40\u0e08\u0e23\u0e34\u0e0d  \u0e04\u0e38\u0e27\u0e34\u0e19\u0e17\u0e23\u0e4c\u0e1e\u0e31\u0e19\u0e18\u0e38\u0e4c - LK82  \u0e01\u0e0e\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e23\u0e2b\u0e31\u0e2a\u0e0b\u0e32\u0e27\u0e19\u0e4c\u0e40\u0e14\u0e47\u0e01\u0e0b\u0e4c\u0e02\u0e2d\u0e07 \u0e27\u0e23\u0e23\u0e13\u0e35 \u0e2d\u0e38\u0e14\u0e21\u0e1e\u0e32\u0e13\u0e34\u0e0a\u0e22\u0e4c - Udom83   \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  >>> from pythainlp.soundex import LK82\n>>> print(LK82('\u0e23\u0e16'))\n\u0e233000\n>>> print(LK82('\u0e23\u0e14'))\n\u0e233000\n>>> print(LK82('\u0e08\u0e31\u0e19'))\n\u0e084000\n>>> print(LK82('\u0e08\u0e31\u0e19\u0e17\u0e23\u0e4c'))\n\u0e084000\n>>> print(Udom83('\u0e23\u0e16'))\n\u0e23800000",
            "title": "Thai Soundex \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#meta-sound",
            "text": "Snae & Br\u00fcckner. (2009). Novel Phonetic Name Matching Algorithm with a Statistical Ontology for Analysing Names Given in Accordance with Thai Astrology. Retrieved from https://pdfs.semanticscholar.org/3983/963e87ddc6dfdbb291099aa3927a0e3e4ea6.pdf  \u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  >>> from pythainlp.MetaSound import *\n>>> MetaSound('\u0e04\u0e19')\n'15'",
            "title": "Meta Sound \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#sentiment-analysis",
            "text": "\u0e43\u0e0a\u0e49\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e32\u0e01  https://github.com/wannaphongcom/lexicon-thai/tree/master/\u0e02\u0e49\u0e2d\u0e04\u0e27\u0e32\u0e21/  from pythainlp.sentiment import sentiment\nsentiment(str)  \u0e23\u0e31\u0e1a\u0e04\u0e48\u0e32 str \u0e2a\u0e48\u0e07\u0e2d\u0e2d\u0e01\u0e40\u0e1b\u0e47\u0e19 pos , neg \u0e2b\u0e23\u0e37\u0e2d neutral",
            "title": "Sentiment analysis \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#util",
            "text": "\u0e01\u0e32\u0e23\u0e43\u0e0a\u0e49\u0e07\u0e32\u0e19  from pythainlp.util import *",
            "title": "Util"
        },
        {
            "location": "/pythainlp-1-4-thai/#ngrams",
            "text": "\u0e2a\u0e33\u0e2b\u0e23\u0e31\u0e1a\u0e2a\u0e23\u0e49\u0e32\u0e07 ngrams   ngrams(token,num)   token \u0e04\u0e37\u0e2d list  num \u0e04\u0e37\u0e2d \u0e08\u0e33\u0e19\u0e27\u0e19 ngrams",
            "title": "ngrams"
        },
        {
            "location": "/pythainlp-1-4-thai/#corpus",
            "text": "",
            "title": "Corpus"
        },
        {
            "location": "/pythainlp-1-4-thai/#stopword",
            "text": "from pythainlp.corpus import stopwords\nstopwords = stopwords.words('thai')",
            "title": "stopword \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#_6",
            "text": "from pythainlp.corpus import country\ncountry.get_data()",
            "title": "\u0e0a\u0e37\u0e48\u0e2d\u0e1b\u0e23\u0e30\u0e40\u0e17\u0e28 \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#_7",
            "text": "from pythainlp.corpus import tone\ntone.get_data()",
            "title": "\u0e15\u0e31\u0e27\u0e27\u0e23\u0e23\u0e13\u0e22\u0e38\u0e01\u0e15\u0e4c\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#_8",
            "text": "from pythainlp.corpus import alphabet\nalphabet.get_data()",
            "title": "\u0e15\u0e31\u0e27\u0e1e\u0e22\u0e31\u0e0d\u0e0a\u0e19\u0e30\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        },
        {
            "location": "/pythainlp-1-4-thai/#_9",
            "text": "from pythainlp.corpus.thaiword import get_data # \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e01\u0e48\u0e32\nget_data()\nfrom pythainlp.corpus.newthaiword import get_data # \u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e43\u0e2b\u0e21\u0e48\nget_data()  \u0e40\u0e02\u0e35\u0e22\u0e19\u0e42\u0e14\u0e22 \u0e19\u0e32\u0e22 \u0e27\u0e23\u0e23\u0e13\u0e1e\u0e07\u0e29\u0e4c  \u0e20\u0e31\u0e17\u0e17\u0e34\u0e22\u0e44\u0e1e\u0e1a\u0e39\u0e25\u0e22\u0e4c",
            "title": "\u0e23\u0e32\u0e22\u0e01\u0e32\u0e23\u0e04\u0e33\u0e43\u0e19\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22"
        }
    ]
}